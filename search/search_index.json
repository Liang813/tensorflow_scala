{"docs":[{"location":"/paradox.json","text":"","title":""},{"location":"/index.html","text":"","title":"TensorFlow Scala"},{"location":"/index.html#tensorflow-scala","text":"This library is a Scala API for https://www.tensorflow.org. It attempts to provide most of the functionality provided by the official Python API, while at the same type being strongly-typed and adding some new features. It is a work in progress and a project I started working on for my personal research purposes. Much of the API should be relatively stable by now, but things are still likely to change.","title":"TensorFlow Scala"},{"location":"/index.html#citation","text":"It would be greatly appreciated if you could cite this project using the following BibTex entry, if you end up using it in your work:\n@misc{Platanios:2018:tensorflow-scala,\n  title        = {{TensorFlow Scala}},\n  author       = {Platanios, Emmanouil Antonios},\n  howpublished = {\\url{https://github.com/eaplatanios/tensorflow_scala}},\n  year         = {2018}\n}","title":"Citation"},{"location":"/index.html#main-features","text":"Easy manipulation of tensors and computations involving tensors (similar to NumPy in Python): val t1 = Tensor(1.2, 4.5)\nval t2 = Tensor(-0.2, 1.1)\nt1 + t2 == Tensor(1.0, 5.6) Low-level graph construction API, similar to that of the Python API, but strongly typed wherever possible: val inputs      = tf.placeholder[Float](Shape(-1, 10))\nval outputs     = tf.placeholder[Float](Shape(-1, 10))\nval predictions = tf.nameScope(\"Linear\") {\n  val weights = tf.variable[Float](\"weights\", Shape(10, 1), tf.ZerosInitializer)\n  tf.matmul(inputs, weights)\n}\nval loss        = tf.sum(tf.square(predictions - outputs))\nval optimizer   = tf.train.AdaGrad(1.0f)\nval trainOp     = optimizer.minimize(loss) Numpy-like indexing/slicing for tensors. For example: tensor(2 :: 5, ---, 1) // is equivalent to numpy's 'tensor[2:5, ..., 1]' High-level API for creating, training, and using neural networks. For example, the following code shows how simple it is to train a multi-layer perceptron for MNIST using TensorFlow for Scala. Here we omit a lot of very powerful features such as summary and checkpoint savers, for simplicity, but these are also very simple to use. // Load and batch data using pre-fetching.\nval dataset = MNISTLoader.load(Paths.get(\"/tmp\"))\nval trainImages = tf.data.datasetFromTensorSlices(dataset.trainImages.toFloat)\nval trainLabels = tf.data.datasetFromTensorSlices(dataset.trainLabels.toLong)\nval trainData =\n  trainImages.zip(trainLabels)\n      .repeat()\n      .shuffle(10000)\n      .batch(256)\n      .prefetch(10)\n\n// Create the MLP model.\nval input = Input(FLOAT32, Shape(-1, 28, 28))\nval trainInput = Input(INT64, Shape(-1))\nval layer = Flatten[Float](\"Input/Flatten\") >>\n    Linear[Float](\"Layer_0\", 128) >> ReLU[Float](\"Layer_0/Activation\", 0.1f) >>\n    Linear[Float](\"Layer_1\", 64) >> ReLU[Float](\"Layer_1/Activation\", 0.1f) >>\n    Linear[Float](\"Layer_2\", 32) >> ReLU[Float](\"Layer_2/Activation\", 0.1f) >>\n    Linear[Float](\"OutputLayer\", 10)\nval loss = SparseSoftmaxCrossEntropy[Float, Long, Float](\"Loss\") >>\n    Mean(\"Loss/Mean\")\nval optimizer = tf.train.GradientDescent(1e-6f)\nval model = Model.simpleSupervised(input, trainInput, layer, loss, optimizer)\n\n// Create an estimator and train the model.\nval estimator = InMemoryEstimator(model)\nestimator.train(() => trainData, StopCriteria(maxSteps = Some(1000000))) And by changing a few lines to the following code, you can get checkpoint capability, summaries, and seamless integration with TensorBoard: override val loss = SparseSoftmaxCrossEntropy[Float, Long, Float](\"Loss\") >>\n    Mean(\"Loss/Mean\") >>\n    ScalarSummary(name = \"Loss\", tag = \"Loss\")\nval summariesDir = Paths.get(\"/tmp/summaries\")\noverride val estimator = InMemoryEstimator(\n  modelFunction = model,\n  configurationBase = Configuration(Some(summariesDir)),\n  trainHooks = Set(\n    SummarySaver(summariesDir, StepHookTrigger(100)),\n    CheckpointSaver(summariesDir, StepHookTrigger(1000))),\n  tensorBoardConfig = TensorBoardConfig(summariesDir))\nestimator.train(() => trainData, StopCriteria(maxSteps = Some(100000))) If you now browse to https://127.0.0.1:6006 while training, you can see the training progress: Efficient interaction with the native library that avoids unnecessary copying of data. All tensors are created and managed by the native TensorFlow library. When they are passed to the Scala API (e.g., fetched from a TensorFlow session), we use a combination of weak references and a disposing thread running in the background. Please refer to tensorflow/src/main/scala/org/platanios/tensorflow/api/utilities/Disposer.scala, for the implementation.","title":"Main Features"},{"location":"/index.html#tutorials","text":"Object Detection using Pre-Trained Models","title":"Tutorials"},{"location":"/index.html#funding","text":"Funding for the development of this library has been generously provided by the following sponsors:\nCMU Presidential Fellowship National Science Foundation Air Force Office of Scientific Research awarded to Emmanouil Antonios Platanios Grant #: IIS1250956 Grant #: FA95501710218\nTensorFlow, the TensorFlow logo, and any related marks are trademarks of Google Inc.","title":"Funding"},{"location":"/installation.html","text":"","title":"Installation"},{"location":"/installation.html#installation","text":"TensorFlow for Scala is currently available for Scala 2.12 and for 2.13. The main line of development is version 2.12.11. Binary release artifacts are published to the Sonatype OSS Repository Hosting service and synced to Maven Central. Currently, given the beta status of this project, only snapshot releases are published.","title":"Installation"},{"location":"/installation.html#library-dependencies","text":"To include the Sonatype repositories in your SBT build and use TensorFlow for Scala, you should add the following dependency:\nsbt libraryDependencies += \"org.platanios\" %% \"tensorflow\" % \"0.4.1\" Maven <dependency>\n  <groupId>org.platanios</groupId>\n  <artifactId>tensorflow_2.13</artifactId>\n  <version>0.4.1</version>\n</dependency> Gradle dependencies {\n  compile group: 'org.platanios', name: 'tensorflow_2.13', version: '0.4.1'\n}\nNote This requires that you have installed the TensorFlow dynamic library in your system. If you haven’t, please continue reading into the following section.\nScala version If you are not using SBT, make sure to change the artifact identifier to match the version of Scala you are using.","title":"Library Dependencies"},{"location":"/installation.html#dependencies","text":"","title":"Dependencies"},{"location":"/installation.html#tensorflow-dynamic-library","text":"TensorFlow for Scala is an API for TensorFlow. In order for it work, it requires that you have the main TensorFlow dynamic library installed. You have two options for dealing with this requirement:","title":"TensorFlow Dynamic Library"},{"location":"/installation.html#using-precompiled-binaries","text":"Add the following dependency, instead of the previous one:\nsbt libraryDependencies += \"org.platanios\" %% \"tensorflow\" % \"0.4.1\" classifier \"linux-cpu-x86_64\" Maven <dependency>\n  <groupId>org.platanios</groupId>\n  <artifactId>tensorflow_2.13</artifactId>\n  <version>0.4.1</version>\n  <classifier>linux-cpu-x86_64</classifier>\n</dependency> Gradle dependencies {\n  compile group: 'org.platanios', name: 'tensorflow_2.13', version: '0.4.1', classifier: 'linux-cpu-x86_64'\n}\nOperating System Make sure to replace linux-cpu-x86_64 with the string that corresponds to your platform.* Currently supported platforms are: linux-cpu-x86_64, linux-gpu-x86_64, and darwin-cpu-x86_64.","title":"Using Precompiled Binaries"},{"location":"/installation.html#compiling-tensorflow-from-scratch","text":"Compile the TensorFlow dynamic library yourself and install it in your system. This is the recommended approach if you care about performance, but it is also significantly more complicated and time consuming.\nFirst, clone the TensorFlow repository:\ngit clone https://github.com/tensorflow/tensorflow.git <repository_directory>\ncd <repository_directory>\ngit checkout 1aaa68d93c6b2f4151446eb211399b4330c96a09\nThen, compile TensorFlow using the following commands:\n./configure\nbazel build --config=opt --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 //tensorflow:libtensorflow.so\nFor details regarding the configuration options (e.g., GPU support), please refer to the relevant main TensorFlow documentation page.\nFinally, copy the bazel-bin/tensorflow/libtensorflow.so file (possibly having a different extension, depending on the platform you’re using) file in a directory that is in LD_LIBRARY_PATH, or set LD_LIBRARY_PATH appropriately.\nCompiling TensorFlow Scala If you want to compile TensorFlow for Scala yourself and the libtensorflow.so file is not placed in one of the default system libraries directories, then set -Djava.library.path=<directory> (replacing <directory> with the directory containing the libtensorflow.so file) in the .jvmopts file at the root of the TensorFlow for Scala repository.","title":"Compiling TensorFlow from Scratch"},{"location":"/installation.html#protocol-buffers-compiler","text":"TensorFlow for Scala also requires protoc, the Protocol Buffers compiler (at least version 3), to be installed. You also have two options for dealing with this requirement.","title":"Protocol Buffers Compiler"},{"location":"/installation.html#using-precompiled-binaries","text":"Download pre-built binaries from https://github.com/google/protobuf/releases/ (choose the protoc variant appropriate for your platform) and make sure that protoc is in the PATH (either by installing it in a location in the PATH, or by adding its location to the PATH).","title":"Using Precompiled Binaries"},{"location":"/installation.html#installing-using-a-package-manager","text":"Install it using a package manager:\nOn Debian/Ubuntu, you can install it with APT, using the following command: apt-get install protobuf-compiler On Mac, you can install with Homebrew, using the following command: brew install protobuf","title":"Installing Using a Package Manager"},{"location":"/guides.html","text":"","title":"Guides"},{"location":"/guides.html#guides","text":"Similar to the TensorFlow Python API, by Google, TensorFlow for Scala provides multiple APIs. The lowest level API – Core API – provides you with complete programming control. the core API is suitable for machine learning researchers and others who require fine levels of control over their models. The higher level APIs are built on top of the Core API. These higher level APIs are typically easier to learn and use. In addition, the higher level APIs make repetitive tasks easier and more consistent between different users. A high-level API like the Learn API helps you manage datasets, models, (distributed) training, and inference.\nThe main APIs of TensorFlow Scala are the following:\nThe fact that this library is statically-typed is mentioned a couple times in the above paragraph and that’s because it is a very important feature. It means that many problems with the code you write will show themselves at compile time, which means that your chances of running into the experience of waiting for a neural network to train for a week only to find out that your evaluation code crashed and you lost everything, decrease significantly.\nIt is recommended to first go through the Tensors guide, and then go from high-level to low-level concepts as you progress (i.e., read through the High-Level Learning guide first and then through the Graph Construction guide). Concepts such as the TensorFlow graph and sessions only appear in the Graph Construction guide.\nRelationship to the TensorFlow Python API These guides borrow a lot of material from the official Python API documentation of TensorFlow and adapt it for the purposes of TensorFlow Scala. They also introduce a lot of new constructs specific to this library.","title":"Guides"},{"location":"/guides/tensors.html","text":"","title":"Tensors"},{"location":"/guides/tensors.html#tensors","text":"TensorFlow, as the name indicates, is a framework used to define and run computations involving tensors. A tensor is a generalization of vectors and matrices to potentially higher dimensions. Internally, TensorFlow represents tensors as n-dimensional arrays of some underlying data type. A Tensor has a DataType (e.g., FLOAT32, which corresponds to 32-bit floating point numbers) and a Shape (that is, the number of dimensions it has and the size of each dimension – e.g., Shape(10, 2) which corresponds to a matrix with 10 rows and 2 columns) associated with it. Each element in the Tensor has the same data type. For example, the following code creates an integer tensor filled with zeros with shape [2, 5] (i.e., a two-dimensional array holding integer values, where the first dimension size is 2 and the second is 5):\nval tensor = Tensor.zeros[Int](Shape(2, 5))\nYou can print the contents of a tensor as follows:\ntensor.summarize()\n// Prints the following:\n//   Tensor[Int, [2, 5]]\n//   [[0, 0, 0, 0, 0],\n//    [0, 0, 0, 0, 0]]","title":"Tensors"},{"location":"/guides/tensors.html#tensor-creation","text":"Tensors can be created using various constructors defined in the Tensor companion object. For example:\nval a = Tensor[Int](1, 2)                  // Creates a Tensor[Int] with shape [2]\nval b = Tensor[Long](1L, 2)                // Creates a Tensor[Long] with shape [2]\nval c = Tensor[Float](3.0f)                // Creates a Tensor[Float] with shape [1]\nval d = Tensor[Double](-4.0)               // Creates a Tensor[Double] with shape [1]\nval e = Tensor.empty[Int]                  // Creates an empty Tensor[Int] with shape [0]\nval z = Tensor.zeros[Float](Shape(5, 2))   // Creates a zeros Tensor[Float] with shape [5, 2]\nval r = Tensor.randn(Double, Shape(10, 3)) // Creates a Tensor[Double] with shape [10, 3] and\n                                           // elements drawn from the standard Normal distribution.","title":"Tensor Creation"},{"location":"/guides/tensors.html#data-types","text":"As already mentioned, tensors have a data type. Various numeric data types are supported, as well as strings (i.e., tensors containing strings are supported). It is not possible to have a Tensor with more than one data type. It is possible, however, to serialize arbitrary data structures as strings and store those in tensors.\nThe list of all supported data types is:\nSTRING     // String\nBOOLEAN    // Boolean\nFLOAT16    // 16-bit half-precision floating-point\nFLOAT32    // 32-bit single-precision floating-point\nFLOAT64    // 64-bit double-precision floating-point\nBFLOAT16   // 16-bit truncated floating-point\nCOMPLEX64  // 64-bit single-precision complex\nCOMPLEX128 // 128-bit double-precision complex\nINT8       // 8-bit signed integer\nINT16      // 16-bit signed integer\nINT32      // 32-bit signed integer\nINT64      // 64-bit signed integer\nUINT8      // 8-bit unsigned integer\nUINT16     // 16-bit unsigned integer\nQINT8      // Quantized 8-bit signed integer\nQINT16     // Quantized 16-bit signed integer\nQINT32     // Quantized 32-bit signed integer\nQUINT8     // Quantized 8-bit unsigned integer\nQUINT16    // Quantized 16-bit unsigned integer\nRESOURCE   // Handle to a mutable resource\nVARIANT    // Variant\nTensorFlow Scala also provides value classes for the types that are not natively supported by Scala (e.g., UByte corresponds to UINT8).\nIt is also possible to cast tensors from one data type to another using the toXXX operator, or the castTo[XXX] operator:\nval floatTensor = Tensor[Float](1, 2, 3) // Floating point vector containing the elements: 1.0f, 2.0f, and 3.0f.\nfloatTensor.toInt                        // Integer vector containing the elements: 1, 2, and 3.\nfloatTensor.castTo[Int]                  // Integer vector containing the elements: 1, 2, and 3.\nA tensor’s data type can be inspected using:\nfloatTensor.dataType // Returns FLOAT32\nPerforming Operations on Tensors In general, all tensor-supported operations can be accessed as direct methods/operators of the Tensor object, or as static methods defined in the tfi package, which stands for TensorFlow Imperative (given the imperative nature of this API).","title":"Data Types"},{"location":"/guides/tensors.html#shape","text":"The shape of a tensor is the number of elements it contains in each dimension. The TensorFlow documentation uses two notational conventions to describe tensor dimensionality: rank, and shape. The following table shows how these relate to one another:\nRank Shape Example 0 [] A 0-D tensor. A scalar. 1 [D0] A 1-D tensor with shape [5]. 2 [D0, D1] A 2-D tensor with shape [3, 4]. 3 [D0, D1, D2] A 3-D tensor with shape [1, 4, 3]. n [D0, D1, … Dn-1] A tensor with shape [D0, D1, … Dn-1].\nShapes can be automatically converted to integer tensors, if necessary.\nNote Shapes are automatically converted to Tensor[Int] and not Tensor[Long] in order to improve performance when working with GPUs. The reason is that TensorFlow treats integer tensors in a special manner, if they are placed on GPUs, assuming that they represent shapes.\nFor example:\nval t0 = Tensor.ones[Int](Shape())     // Creates a scalar equal to the value 1\nval t1 = Tensor.ones[Int](Shape(10))   // Creates a vector with 10 elements, all of which are equal to 1\nval t2 = Tensor.ones[Int](Shape(5, 2)) // Creates a matrix with 5 rows with 2 columns\n\n// You can also create tensors in the following way:\nval t3 = Tensor(2.0, 5.6)                                 // Creates a vector that contains the numbers 2.0 and 5.6\nval t4 = Tensor(Tensor(1.2f, -8.4f), Tensor(-2.3f, 0.4f)) // Creates a matrix with 2 rows and 2 columns\nThe shape of a tensor can be inspected using:\nt4.shape // Returns the value Shape(2, 2)","title":"Shape"},{"location":"/guides/tensors.html#rank","text":"The rank of of a tensor is its number of dimensions. Synonyms for rank include order or degree or n-dimension. Note that rank in TensorFlow is not the same as matrix rank in mathematics. As the following table shows, each rank in TensorFlow corresponds to a different mathematical entity:\nRank Math Entity 0 Scalar (magnitude only) 1 Vector (magnitude and direction) 2 Matrix (table of numbers) 3 3-Tensor (cube of numbers) n n-Tensor (you get the idea)\nThe rank of a tensor can be inspected using:\nt4.rank // Returns the value 2","title":"Rank"},{"location":"/guides/tensors.html#indexing-slicing","text":"Similar to NumPy, tensors can be indexed/sliced in various ways. An indexer can be one of:\nEllipsis: Full slice over multiple dimensions of a tensor. Ellipses are used to represent zero or more dimensions of a full-dimension indexer sequence. NewAxis: Addition of a new dimension. Slice: Slice over a single dimension of a tensor.\nExamples of constructing and using indexers are provided in the Ellipsis and the Slice documentation. Here we provide examples of indexing over tensors using indexers:\nval t = Tensor.zeros[Float](Shape(4, 2, 3, 8))\nt(::, ::, 1, ::)            // Tensor with shape [4, 2, 1, 8]\nt(1 :: -2, ---, 2)          // Tensor with shape [1, 2, 3, 1]\nt(---)                      // Tensor with shape [4, 2, 3, 8]\nt(1 :: -2, ---, NewAxis, 2) // Tensor with shape [1, 2, 3, 1, 1]\nt(1 ::, ---, NewAxis, 2)    // Tensor with shape [3, 2, 3, 1, 1]\nwhere --- corresponds to an ellipsis.\nNote that each indexing sequence is only allowed to contain at most one ellipsis. Furthermore, if an ellipsis is not provided, then one is implicitly appended at the end of the indexing sequence. For example, foo(2 :: 4) is equivalent to foo(2 :: 4, ---).","title":"Indexing / Slicing"},{"location":"/guides/graph_construction.html","text":"","title":"Graph Construction"},{"location":"/guides/graph_construction.html#graph-construction","text":"The low level API can be used to define computations that will be executed at a later point, and potentially execute them. It can also be used to create custom layers for the Learn API. The main type of object underlying the low level API is the [Output][output], which represents the value of a [Tensor][tensor] that has not yet been computed. Its name comes from the fact that it represents the output of some computation. An [Output][output] object thus represents a partially defined computation that will eventually produce a value. Core TensorFlow programs work by first building a graph of [Output][output] objects, detailing how each output is computed based on the other available outputs, and then by running parts of this graph to achieve the desired results.\nSimilar to a [Tensor][tensor], each element in an [Output][output] has the same data type, and the data type is always known. However, the shape of an [Output][output] might be only partially known. Most operations produce tensors of fully-known shapes if the shapes of their inputs are also fully known, but in some cases it’s only possible to find the shape of a tensor at graph execution time.\nIt is important to understand the main concepts underlying the core API:\nTensor: Output: Sparse Output: Placeholder: Variable: Graph: Session:\nWith the exception of [Variable][variable]s, the value of outputs is immutable, which means that in the context of a single execution, outputs only have a single value. However, evaluating the same output twice can result in different values. For example, that tensor may be the result of reading data from disk, or generating a random number.","title":"Graph Construction"},{"location":"/guides/graph_construction.html#graph","text":"","title":"Graph"},{"location":"/guides/graph_construction.html#working-with-outputs","text":"","title":"Working with Outputs"},{"location":"/guides/graph_construction.html#evaluating-outputs","text":"","title":"Evaluating Outputs"},{"location":"/guides/graph_construction.html#printing-outputs","text":"","title":"Printing Outputs"},{"location":"/guides/graph_construction.html#logging","text":"Logging in the native TensorFlow library can be controlled by setting the TF_CPP_MIN_LOG_LEVEL environment variable:\n0: Debug level (default). 1: Warning level. 2: Error level. 3: Fatal level.","title":"Logging"},{"location":"/guides/estimators.html","text":"","title":"Estimators"},{"location":"/guides/estimators.html#estimators","text":"","title":"Estimators"},{"location":"/guides/estimators.html#models","text":"A Model needs to implement one or more of the following methods, depending on their type:\nInference Models only need to implement a method for building all ops used for inference: trait InferenceModel[In, Out] extends Model {\n  def buildInferOps(): Model.InferOps[In, Out]\n} Trainable Models also need to implement a method for building all ops used for training, and one for building all ops used for evaluation: trait TrainableModel[In, TrainIn, Out, TrainOut, Loss, EvalIn] extends InferenceModel[In, Out] {\n  def buildTrainOps(): Model.TrainOps[TrainIn, TrainOut, Loss]\n  def buildEvalOps(metrics: Seq[Metric[EvalIn, Output[Float]]]): Model.EvalOps[TrainIn, Out]\n}\n\ntrait SupervisedTrainableModel[In, TrainIn, Out, TrainOut, Loss] extends TrainableModel[In, (In, TrainIn), Out, TrainOut, Loss, (Out, (In, TrainIn))] {\n  override def buildTrainOps(): Model.TrainOps[(In, TrainIn), TrainOut, Loss]\n  override def buildEvalOps(metrics: Seq[Metric[(Out, (In, TrainIn)), Output[Float]]]): Model.EvalOps[(In, TrainIn), Out]\n}\n\ntrait UnsupervisedTrainableModel[In, Out, Loss] extends TrainableModel[In, In, Out, Out, Loss, Out] {\n  override def buildTrainOps(): Model.TrainOps[In, Out, Loss]\n  override def buildEvalOps(metrics: Seq[Metric[Out, Output[Float]]]): Model.EvalOps[In, Out]\n}","title":"Models"},{"location":"/guides/estimators.html#type-parameters","text":"The type parameters of these classes can be broadly interpreted as follows:\nIn: Input type of the model, at inference time (i.e., when the model is being used/deployed). TrainIn: Input type of the model, at training time (e.g., this could be the model inference inputs along with supervision labels). Out: Output type of the model, at inference time. TrainOut: Output type of the model, at training time. Note that this is useful to keep separate from Out. From example, during training a model may output a distribution over some labels whereas during inference it may output the single label with the highest probability. Loss: Type of the loss function value (e.g., Float). EvalIn: Input type of the model, at evaluation time.","title":"Type Parameters"},{"location":"/guides/estimators.html#ops-classes","text":"Here we describe the ops classes that the buildXXXOps() methods defined above return. In summary, these classes are wrappers over constructed ops that can be used to perform inference, training, and evaluation, using models.\nThe InferOps class contains all ops that are used for inference:\ninputIterator: DatasetIterator[In]: Dataset iterator. input: In: Retrieved element from the dataset iterator. output: Out: Model inference output.\nThe TrainOps class contains all ops that are used for training:\ninputIterator: DatasetIterator[TrainIn]: Train dataset iterator. input: TrainIn: Retrieved element from the dataset iterator. output: TrainOut: Model training output. loss: Output[Loss]: Scalar tensor containing the loss value. gradientsAndVariables: Seq[(OutputLike[Loss], Variable[Any])]: Gradients of the loss along with their corresponding variables. trainOp: UntypedOp: Op that when executed performs a single training step (e.g., a gradient descent step for a single batch).\nThe EvalOps class contains all ops that are used for evaluation:\ninputIterator: DatasetIterator[In]: Dataset iterator. input: In: Retrieved element from the dataset iterator. output: Out: Model evaluation output. metricValues: Seq[Output[Float]]: Metric value ops. metricUpdates: Seq[Output[Float]]: Metric update ops. metricResets: Set[UntypedOp]: Metric reset ops.\nNote that the metrics have value, update, and reset ops, because estimators only use streaming versions of the metrics.","title":"Ops Classes"},{"location":"/guides/adding_ops.html","text":"","title":"Adding Support for New Ops"},{"location":"/guides/adding_ops.html#adding-support-for-new-ops","text":"TensorFlow graphs are constructed by buildings ops that receive tensors as input and produce tensors as output. Internally, multiple kernels are registered for each op, which are implementations of the op for different architectures (e.g., CPU kernels, CUDA GPU kernels, etc.). TensorFlow Scala offers the Op.Builder interface to allow users to create arbitrary ops that the TensorFlow runtime supports.\nFor example, the implentation of tf.add(x, y) in TensorFlow Scala looks like this:\ndef add[T: TF : IsNotQuantized](\n    x: Output[T],\n    y: Output[T],\n    name: String = \"Add\"\n): Output[T] = {\n  Op.Builder[(Output[T], Output[T]), Output[T]](\n    opType = \"Add\",\n    name = name,\n    input = (x, y)\n  ).setGradientFn(addGradient(_, _)(TF[T], IsNotQuantized[T]))\n      .build().output\n}\n\nprotected def addGradient[T: TF : IsNotQuantized](\n    op: Op[(Output[T], Output[T]), Output[T]],\n    outputGradient: Output[T]\n): (Output[T], Output[T]) = {\n  val xShape = tf.shape(op.input._1)\n  val yShape = tf.shape(op.input._2)\n  val (rx, ry) = tf.broadcastGradientArguments(xShape, yShape)\n  (tf.reshape(tf.sum(outputGradient, rx), xShape),\n      tf.reshape(tf.sum(outputGradient, ry), yShape))\n}","title":"Adding Support for New Ops"},{"location":"/contributing.html","text":"","title":"Contributing"},{"location":"/contributing.html#contributing","text":"It would be awesome if people could contribute to this library. Given its scope and its early state, before I settle on the API for some of the features, I would really appreciate contributions on the following:\nUnit Tests: Currently unit tests are missing for a big part of the library and it would be extremely useful if we had those. Examples: Examples of code using the library would be great and would also make issues come up early so they can be fixed.","title":"Contributing"},{"location":"/release_notes.html","text":"","title":"Release Notes"},{"location":"/release_notes.html#release-notes","text":"Release 0.5.1 Release 0.5.0 Release 0.4.1 Release 0.4.0 Release 0.3.0 Release 0.2.4 Release 0.2.3 Release 0.2.2 Release 0.2.1 Release 0.2.0 Release 0.1.1 Release 0.1.0","title":"Release Notes"},{"location":"/release_notes/0.5.1.html","text":"","title":"Release 0.5.1"},{"location":"/release_notes/0.5.1.html#release-0-5-1","text":"This release introduces support for TensorFlow 2.2 and Scala 2.13 and drops support for Scala 2.11. The distributed precompiled binaries for this version will only work with CUDA 10.1 on Linux. Finally, this release also brings improved support for implicit derivations in some cases where case classes over tensors are used.","title":"Release 0.5.1"},{"location":"/release_notes/0.5.0.html","text":"","title":"Release 0.5.0"},{"location":"/release_notes/0.5.0.html#release-0-5-0","text":"This release introduces support for TensorFlow 2.0.","title":"Release 0.5.0"},{"location":"/release_notes/0.4.1.html","text":"","title":"Release 0.4.1"},{"location":"/release_notes/0.4.1.html#release-0-4-1","text":"Fixed the precompiled TensorFlow binaries, and also added the following new features:\nio module: Added support for a new TFRecordWriter. ops module: Added a new ops namespace, sparse, that includes all sparse ops. Added support for sparse.reorder and sparse.merge. Added support for parsing TF records. data module: Added support for Dataset.shuffleAndRepeat. optimizers module: Added support for the Adafactor optimizer. Renamed SqrtDecay to RSqrtDecay which is more appropriate. math module: Added support for batchGather. Added support for bitwise ops. rnn module: Simplified the attention mechanisms functionality so that it is now not required to tile memory tensors for beam search outside the beam search decoder. Moved the seq2seq module to a separate repository (that of Symphony Machine Translation).","title":"Release 0.4.1"},{"location":"/release_notes/0.4.0.html","text":"","title":"Release 0.4.0"},{"location":"/release_notes/0.4.0.html#release-0-4-0","text":"This is a major release with a lot of new features related to static types for tensors and ops. The graph construction API is now statically-typed, thus enabling much better type safety than before.\nTensors and outputs are now statically-typed and the types used are the Scala types that correspond to the tensors’ TensorFlow data types. For example:\nval t1 = Tensor(0.5, 1) // The inferred type is Tensor[Double].\nval t2 = Tensor(1, 2)   // The inferred type is Tensor[Int].\nval t3 = t1 + t2        // The inferred type is Tensor[Double].\nval t4 = t3.isNaN       // The inferred type is Tensor[Boolean].\nval t5 = t3.any()       // Fails at compile-time because `any()` is only\n                        // supported for Tensor[Boolean].\nA similar situation now applies to Outputs. Ops are also typed and so is the auto-differentiation implementation.\nThis resulted in major simplifications in the data pipeline and the high level learn API. Datasets and dataset iterators do not “carry” T, V, D, and S types with them now, but rather just the type of the elements they contain/produce.\nA new type trait called TF is also introduced that denotes supported Scala types in TensorFlow (e.g., TF[Int] and TF[Float]). Similarly, some more type traits are introduced to denote type constraints for various ops (e.g., IsIntOrUInt[Int], IsIntOrUInt[Long], IsFloatOrDouble[Float], etc.). These type traits are powered by a general implementation of union types for Scala.\nOther new features include:\ndata module: Added support for the mapAndBatch transformation.","title":"Release 0.4.0"},{"location":"/release_notes/0.3.0.html","text":"","title":"Release 0.3.0"},{"location":"/release_notes/0.3.0.html#release-0-3-0","text":"With this release we have finally added support for static data type information for tensors (not for symbolic tensors yet though – for now we effectively have support for a statically-typed version of numpy for Scala). This is an important milestone and contributes significantly to type safety, which can help catch errors at compile time, rather than runtime. For example:\nval t1 = Tensor(0.5, 1) // The inferred type is Tensor[FLOAT64].\nval t2 = Tensor(1, 2)   // The inferred type is Tensor[INT32].\nval t3 = t1 + t2        // The inferred type is Tensor[FLOAT64].\nval t4 = t3.isNaN       // The inferred type is Tensor[BOOLEAN].\nval t5 = t3.any()       // Fails at compile-time because `any()` is only\n                        // supported for Tensor[BOOLEAN].\nOther new features include:\nImprovements to the high-level learn API: Layers can now provide and use their own parameter generator, and can also access the current training step (using Layer.currentStep). Layers now support .map(...). Added support for batch normalization. Added support for tf.logSigmoid and tf.lrn. Added support for the following new metrics: Grouped precision. Precision-at-k. data module: Added support for loading the extreme classification repository datasets (i.e., data.XCLoader). Added support for randomly splitting datasets.","title":"Release 0.3.0"},{"location":"/release_notes/0.2.4.html","text":"","title":"Release 0.2.4"},{"location":"/release_notes/0.2.4.html#release-0-2-4","text":"Fixed an issue with the packaged pre-compiled TensorFlow binaries that affected Linux platforms.","title":"Release 0.2.4"},{"location":"/release_notes/0.2.3.html","text":"","title":"Release 0.2.3"},{"location":"/release_notes/0.2.3.html#release-0-2-3","text":"Added compatibility with TensorFlow 1.9-rc1.","title":"Release 0.2.3"},{"location":"/release_notes/0.2.2.html","text":"","title":"Release 0.2.2"},{"location":"/release_notes/0.2.2.html#release-0-2-2","text":"In this release we have updated the precompiled TensorFlow binaries distributed with this library.","title":"Release 0.2.2"},{"location":"/release_notes/0.2.1.html","text":"","title":"Release 0.2.1"},{"location":"/release_notes/0.2.1.html#release-0-2-1","text":"In this release we have fixed an issue related to the packaging and distributing of the pre-compiled TensorFlow shared libraries.","title":"Release 0.2.1"},{"location":"/release_notes/0.2.0.html","text":"","title":"Release 0.2.0"},{"location":"/release_notes/0.2.0.html#release-0-2-0","text":"In this release we have:\nAdded support for incremental compilation. Added support for Horovod. Added support for timelines to allow for easy profiling of TensorFlow graphs. Fixed a major memory leak (issue #87). Updated the JNI bindings to be compatible with the TensorFlow 1.9.0 release. Added support for obtaining the list of available devices from within Scala. Fixed bugs for some control flow ops. Added support for tf.cases. Added support for the RMSProp optimizer, the lazy Adam optimizer, the AMSGrad optimizer, the lazy AMSGrad optimizer, and the YellowFin optimizer. Added more learning rate decay schemes: Cosine decay. Cycle-linear 10x decay. Square-root decay. More warm-up decay schedules. Added support for dataset interleave ops. Fixed some bugs related to variable scopes and variable sharing. Fixed some bugs related to functional ops. Added support for some new image-related ops, under the namespace tf.image. Improved consistency for the creation of initializer ops. Added support for the tf.initializer op creation context. Exposed part of the TensorArray API. Exposed tf.Op.Builder in the public API. Improvements to the learn API: Refactored mode into an implicit argument. Improved the evaluator hook. Removed the layer creation context mechanism, to be refactored later. It was causing some issues due to bad design and unclear semantics. The plan is to implement this, in the near future, as wrapper creation context layers. Improved the Model class. Fixed a bug that was causing some issues related to inference hooks in the in-memory estimator. Improved logging. Added support for reading and writing numpy (i.e., .npy) files. Added a logo. :)","title":"Release 0.2.0"},{"location":"/release_notes/0.1.1.html","text":"","title":"Release 0.1.1"},{"location":"/release_notes/0.1.1.html#release-0-1-1","text":"This release fixes the following bugs:\nIssue with the packaged pre-compiled TensorFlow binaries that affected Linux platforms. Learn API bug where the shared name of input iterators was being set incorrectly.\nI also switched to using CircleCI for continuous integration, instead of TravisCI.","title":"Release 0.1.1"},{"location":"/release_notes/0.1.0.html","text":"","title":"Release 0.1.0"},{"location":"/release_notes/0.1.0.html#release-0-1-0","text":"This is the first official release of TensorFlow for Scala. The library website will soon be updated with information about the functionality supported by this API. Most of the main TensorFlow Python API functionality is already supported.","title":"Release 0.1.0"}]}